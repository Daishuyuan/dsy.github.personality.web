---
title: 强化学习
date: 2018-03-11 15:58:17
toc: true
tags:
    - 算法
    - Javascript
---
![围棋与强化学习][1]
***&emsp;&emsp;"You will never reach your destination if you stop and throw stones at every dog that barks." -- Churchill***
# 强化学习 #
## 前言 ##
> &emsp;&emsp;以下内容仅仅是对莫烦python课程学习的总结与回顾，想要了解详细内容请移步[莫烦Python课程][2]。前段时间自学python的时候，无意之间打开了莫烦的教育课程，个人感觉说得非常浅显易懂，对已经完成python学习和相关机器学习课程的人来说是一种莫大的鼓舞。当然，必须承认，这种鼓舞只是感觉自己也能像莫烦一样可以快速实现一个想要的机器学习效果。然而，事实总是残酷的，学习编程的过程不光是算法的考验，也是对编程细节的把控能力的锻炼，在不断debug中学习和进步。之后的实践部分，我使用了JavaScript语言，重写了莫烦的python代码。

## 基本概念 ##
> &emsp;&emsp;机器学习可以分为三类，分别是 **supervised learning**，**unsupervised learning** 和 **reinforcement learning**。而强化学习与其他机器学习不同之处在于，强化学习有一个相对吝啬的老师，他并不提供任何指导，而只对程序当前的行为进行打分（rewards），而其他的机器学习算法，需要有标记的数据进行训练。三种机器学习的区别如下：
- ***Surpervised Learning*** : given **data**, predict **labels**
- ***Unsurpervised Learning*** : given **data**, learn about that **data**
- ***Reinforcement Learning*** : given **data**, choose **action** to maximize expected **long-term reward**
![强化学习示意图][3]
> &emsp;&emsp;强化学习的关键要素在于四个，**环境（environment）**、**奖励（reward）**、**行为（action）**、**状态（state）**,强化学习的任务通常用马尔可夫决策过程（Markov Decision Process，简称MDP）来描述：机器处于环境E中，状态空间为X，其中每个状态x∈X是机器感受到的环境的描述。

<!-- more -->

# Q-Learning(off-policy) #
## Q-Learning的算法流程 ##
![QLearning算法流程图][4]
> &emsp;&emsp;在s状态时，跟据当前Q网络以及一定的策略（e-greedy）来选取动作a，进而观测到下一状态s'，并再次根据当前Q网络计算出 接下来采取哪个动作会得到最大的Q值，用这个Q值作为当前状态动作对Q值的target。这样就有了一个<s，a，r，s'>序列，成为一个sample。
** 注意：**
1. 在状态s'时，只是计算了在s'时要采取哪个a'能得到更大的Q值，并没有执行；
2. QLearning既可以自己学，也可以看着别人学，而且必须积累一定的经验才能开始学习，所以是off-policy学习；

# SARSA(on-policy) #
## SARSA的算法流程 ##
![SARSA算法流程图][5]
> &emsp;&emsp;在s状态时，根据当前Q网络以及一定的策略（e-greedy)来选取动作a，进而观测到下一个状态s'，并再次根据当前Q网络及相同的e-greedy策略选择动作a'，这样就有了一个<s,a,r,s',a'>序列，成为一个sample。
** 注意：**
1. 在状态s'时，就知道了要采取哪个a'，并真的采取了这个动作。
2. 动作a的选取遵循e-greedy策略，目标Q值的计算也是根据（e-greedy）策略得到的动作a'计算得来，因此为on-policy学习。

# Deep Q Learning #
> 暂无

  [1]: /assets/blogImg/blog-RL-background.jpg
  [2]: https://morvanzhou.github.io/
  [3]: /assets/blogImg/blog-RL-intro.jpg
  [4]: /assets/blogImg/blog-RL-QLearning.png
  [5]: /assets/blogImg/blog-RL-SARSA.png

